{"version":"1","records":[{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl2":"Motivation"},"content":"While there are many dedicated packages to perform Bayesian estimation, it might be very useful to understand what’s under the hood, so scientists and students are increasingly knowledgeable and empowered to make the best modelling decisions. This cookbook is an effort to demonstrate how Monte Carlo Markov Chain (MCMC) algorithms play a role in Bayesian estimation. I have included several applications of this algorithm to different models - starting from estimating basic normal distribution parameters and regressions, all the way to more complex population dynamics models.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl2":"Authors"},"content":"Matheus de Barros","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl3":"Background","lvl2":"Authors"},"type":"lvl3","url":"/#background","position":6},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl3":"Background","lvl2":"Authors"},"content":"","type":"content","url":"/#background","position":7},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl4":"Bayes’ rule and probability statements","lvl3":"Background","lvl2":"Authors"},"type":"lvl4","url":"/#bayes-rule-and-probability-statements","position":8},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl4":"Bayes’ rule and probability statements","lvl3":"Background","lvl2":"Authors"},"content":"","type":"content","url":"/#bayes-rule-and-probability-statements","position":9},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl3":"Bayes’ Rule and Probability Statements","lvl2":"Authors"},"type":"lvl3","url":"/#bayes-rule-and-probability-statements-1","position":10},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl3":"Bayes’ Rule and Probability Statements","lvl2":"Authors"},"content":"In Bayesian models, the goal is to estimate the posterior distribution given a prior distribution p(θ) (what we already know about the parameters), and the likelihood p(y|θ) (the probability of observing the data y given parameter values θ).\n\nTo estimate model parameters θ given data y, we specify the joint probability distribution:\n\np(y, θ) = p(y|θ) p(θ)\n\nApplying Bayes’ rule:\n\np(θ|y) = [p(y|θ) p(θ)] / p(y)\n\nwhere p(y) = ∫ p(y|θ) p(θ) dθ is the marginal likelihood (normalizing constant).\n\nThe unnormalized posterior can be written as:\n\np(θ|y) ∝ p(θ) p(y|θ)","type":"content","url":"/#bayes-rule-and-probability-statements-1","position":11},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl4":"The Metropolis-Hastings Algorithm","lvl3":"Bayes’ Rule and Probability Statements","lvl2":"Authors"},"type":"lvl4","url":"/#the-metropolis-hastings-algorithm","position":12},{"hierarchy":{"lvl1":"Bayesian Monte Carlo Markov Chain modelling Cookbook","lvl4":"The Metropolis-Hastings Algorithm","lvl3":"Bayes’ Rule and Probability Statements","lvl2":"Authors"},"content":"Steps:\n\nInitialization: Start with an initial value θ(0).\n\nProposal: At iteration t, propose θ* from q(θ* | θ(t-1)).\n\nAcceptance Ratio: Compute\n\nα = min(1, [p(θ*) q(θ(t-1)|θ*)] / [p(θ(t-1)) q(θ*|θ(t-1))])\n\nAccept or Reject: Draw u ~ Uniform(0,1).\n\nIf u < α, set θ(t) = θ*.\n\nElse, θ(t) = θ(t-1).\n\nIterate: Repeat steps 2–4 as needed.\n\nKey Properties:\n\nThe sequence {θ(t)} forms a Markov chain with stationary distribution p(θ).\n\nOnly the unnormalized target distribution is needed.\n\nAfter burn-in, use remaining samples for posterior inference.","type":"content","url":"/#the-metropolis-hastings-algorithm","position":13},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo.","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression"},"type":"lvl1","url":"/notebooks/logit-mcmc","position":0},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression"},"content":"","type":"content","url":"/notebooks/logit-mcmc","position":1},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression"},"type":"lvl1","url":"/notebooks/logit-mcmc#a-mcmc-example-for-a-baysian-multiple-logistic-regression","position":2},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression"},"content":"This algorithm samples from the posterior distribution of the regression coefficients \\beta in a Bayesian logistic regression model with a flat (uniform) prior.","type":"content","url":"/notebooks/logit-mcmc#a-mcmc-example-for-a-baysian-multiple-logistic-regression","position":3},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression","lvl2":"Model Setup"},"type":"lvl2","url":"/notebooks/logit-mcmc#model-setup","position":4},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression","lvl2":"Model Setup"},"content":"Likelihood:\nFor binary data y_i \\in \\{0, 1\\}, and predictors X_i, the model is:y_i \\sim \\mathrm{Bernoulli}(p_i), \\quad p_i = \\frac{1}{1 + \\exp(-\\eta_i)}, \\quad \\eta_i = X_i \\beta\n\nThe likelihood for all data is:p(\\mathbf{y} | \\beta, X) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1-y_i}\n\nLog-Likelihood (as implemented in logit_loglik):\\log p(\\mathbf{y} | \\beta, X) = \\sum_{i=1}^n y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\n\nIn code:\n\n\\log(p_i) = \\log\\left(\\frac{1}{1 + \\exp(-\\eta_i)}\\right)\n\n\\log(1 - p_i) = \\log\\left(\\frac{1}{1 + \\exp(\\eta_i)}\\right)\n\nPrior:\n\nFlat prior: p(\\beta) \\propto 1\n\nThe posterior is proportional to the likelihood alone.\n\nPosterior:p(\\beta | \\mathbf{y}, X) \\propto p(\\mathbf{y} | \\beta, X)","type":"content","url":"/notebooks/logit-mcmc#model-setup","position":5},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression","lvl2":"Algorithm Steps and Equations"},"type":"lvl2","url":"/notebooks/logit-mcmc#algorithm-steps-and-equations","position":6},{"hierarchy":{"lvl1":"A MCMC example for a Baysian multiple logistic regression","lvl2":"Algorithm Steps and Equations"},"content":"Proposal Step:\n\nPropose \\beta^* \\sim \\mathcal{N}(\\beta^{(t-1)},\\, \\delta\\,V) where V is the estimated covariance from the frequentist fit.\n\nAcceptance Ratio:\n\nWith a flat prior, the Metropolis-Hastings acceptance ratio simplifies to:\nR = \\frac{p(\\mathbf{y} | \\beta^*, X)}{p(\\mathbf{y} | \\beta^{(t-1)}, X)}\nThe code works on the log scale for stability:\n\\log R = \\log p(\\mathbf{y} | \\beta^*, X) - \\log p(\\mathbf{y} | \\beta^{(t-1)}, X)\n\nAccept \\beta^* with probability \\min(1, R).\n\nSampling:\n\nIf accepted, set \\beta^{(t)} = \\beta^*. Otherwise, set \\beta^{(t)} = \\beta^{(t-1)}.\n\nRepeat for nIter iterations, discarding the first nBurnIn as burn-in.\n\nimport numpy as np\nfrom scipy.stats import multivariate_normal, gaussian_kde\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import gaussian_kde\n\ndef logit_loglik(beta, y, X):\n    \"\"\"\n    Computes log-likelihood for a logit model.\n    beta: shape (p,) or (n,p)\n    y: binary response, shape (n,)\n    X: design matrix, shape (n,p)\n    Returns: log-likelihood\n    \"\"\"\n    beta = np.atleast_2d(beta)\n    n = beta.shape[0]\n    pll = np.zeros(n)\n    for i in range(n):\n        eta = X @ beta[i]\n        lf1 = np.log(1 / (1 + np.exp(-eta)))\n        lf2 = np.log(1 / (1 + np.exp(eta)))\n        pll[i] = np.sum(y * lf1 + (1 - y) * lf2)\n    return pll if n > 1 else pll[0]\n\ndef MHflatLogit(nIter, nBurnIn, y, X, delta, plot=True):\n    \"\"\"\n    Metropolis-Hastings for logistic regression with flat prior.\n    \"\"\"\n    if nIter <= nBurnIn:\n        raise ValueError(\"Number of warm-up iterations must be smaller than total iterations.\")\n\n    n, p = X.shape\n\n    # Frequentist initialization using statsmodels\n    import statsmodels.api as sm\n    model = sm.Logit(y, X)\n    result = model.fit(disp=0)\n    beta_start = result.params\n    variance = result.cov_params()\n\n    beta = np.zeros((nIter, p))\n    beta[0] = beta_start\n\n    for i in range(1, nIter):\n        beta_hat = multivariate_normal.rvs(mean=beta[i-1], cov=delta * variance)\n        # Log acceptance ratio\n        logR = logit_loglik(beta_hat, y, X) - logit_loglik(beta[i-1], y, X)\n        if np.random.uniform() <= np.exp(logR):\n            beta[i] = beta_hat\n        else:\n            beta[i] = beta[i-1]\n\n    # Remove warm-up iterations\n    beta_post = beta[nBurnIn:]\n\n    # Plotting\n    if plot:\n        fig, axs = plt.subplots(2, p, figsize=(4*p, 8))\n        # Traceplots\n        for j in range(p):\n            axs[0, j].plot(beta_post[:, j])\n            axs[0, j].set_title(f'Traceplot: beta{j}' if j > 0 else 'Traceplot: alpha')\n        # Density plots\n        for j in range(p):\n            axs[1, j].hist(beta_post[:, j], bins=50, density=True, alpha=0.6)\n            kde = gaussian_kde(beta_post[:, j])\n            x_vals = np.linspace(np.min(beta_post[:, j]), np.max(beta_post[:, j]), 200)\n            axs[1, j].plot(x_vals, kde(x_vals), 'r')\n            axs[1, j].set_title(f'Density: beta{j}' if j > 0 else 'Density: alpha')\n        plt.tight_layout()\n        plt.show()\n\n    # Posterior summaries\n    param_names = [f'alpha'] + [f'beta{j}' for j in range(1, p)]\n    summary = pd.DataFrame({\n        'Parameter': param_names,\n        'Median': np.median(beta_post, axis=0),\n        'SD': np.std(beta_post, axis=0)\n    })\n\n    # Set column names\n    beta_post_df = pd.DataFrame(beta_post, columns=param_names)\n\n    output = {\n        'posteriorSummary': summary,\n        'posteriorSamples': beta_post_df\n    }\n    return output\n\n### Example usage ###\nnp.random.seed(42)\nn = 300\nX = np.column_stack((np.ones(n), np.random.normal(size=n), np.random.normal(size=n)))\ntrue_beta = np.array([0.5, 1.2, -0.7])\nlogit_p = 1 / (1 + np.exp(-(X @ true_beta)))\ny = (np.random.uniform(size=n) < logit_p).astype(int)\n\nresult = MHflatLogit(nIter=10000, nBurnIn=500, y=y, X=X, delta=0.5, plot=True)\nprint(result['posteriorSummary'])\n\n","type":"content","url":"/notebooks/logit-mcmc#algorithm-steps-and-equations","position":7},{"hierarchy":{"lvl1":"Adding a prior distribution"},"type":"lvl1","url":"/notebooks/logit-mcmc#adding-a-prior-distribution","position":8},{"hierarchy":{"lvl1":"Adding a prior distribution"},"content":"","type":"content","url":"/notebooks/logit-mcmc#adding-a-prior-distribution","position":9},{"hierarchy":{"lvl1":"Adding a prior distribution","lvl2":"A Multivariate Normal Prior"},"type":"lvl2","url":"/notebooks/logit-mcmc#a-multivariate-normal-prior","position":10},{"hierarchy":{"lvl1":"Adding a prior distribution","lvl2":"A Multivariate Normal Prior"},"content":"A common choice for the prior on \\beta is the multivariate normal (Gaussian) distribution:\\beta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\n\nwhere:\n\n\\mu_0 is the prior mean vector (typically a vector of zeros for a weakly informative or “neutral” prior).\n\n\\Sigma_0 is the prior covariance matrix (often diagonal, with large values for weakly informative priors, or small values for strong regularization).\n\nThe density function is:p(\\beta) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma_0|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\beta - \\mu_0)^\\top \\Sigma_0^{-1} (\\beta - \\mu_0)\\right)","type":"content","url":"/notebooks/logit-mcmc#a-multivariate-normal-prior","position":11},{"hierarchy":{"lvl1":"Adding a prior distribution","lvl2":"Role in Posterior Sampling"},"type":"lvl2","url":"/notebooks/logit-mcmc#role-in-posterior-sampling","position":12},{"hierarchy":{"lvl1":"Adding a prior distribution","lvl2":"Role in Posterior Sampling"},"content":"In the Metropolis-Hastings algorithm, the prior is combined with the likelihood to determine the posterior (in this case, the unormalized posterior):p(\\beta | \\mathbf{y}, X) \\propto p(\\mathbf{y} | \\beta, X) \\cdot p(\\beta)\n\nThis means that higher prior density values for certain \\beta make those values more likely to be sampled, all else equal. A normal prior shrinks coefficients toward the prior mean and helps regularize the model, especially when data are sparse or predictors are highly correlated.\n\ndef logit_loglik(beta, y, X):\n    \"\"\"\n    Computes log-likelihood for a logit model.\n    beta: shape (p,) or (n,p)\n    y: binary response, shape (n,)\n    X: design matrix, shape (n,p)\n    Returns: log-likelihood\n    \"\"\"\n    beta = np.atleast_2d(beta)\n    n = beta.shape[0]\n    pll = np.zeros(n)\n    for i in range(n):\n        eta = X @ beta[i]\n        lf1 = np.log(1 / (1 + np.exp(-eta)))\n        lf2 = np.log(1 / (1 + np.exp(eta)))\n        pll[i] = np.sum(y * lf1 + (1 - y) * lf2)\n    return pll if n > 1 else pll[0]\n\ndef MHLogitNormalPrior(nIter, nBurnIn, y, X, delta, prior_mean, prior_cov, plot=True):\n    \"\"\"\n    Metropolis-Hastings for logistic regression with multivariate normal prior.\n    prior_mean: shape (p,)\n    prior_cov: shape (p, p)\n    \"\"\"\n    if nIter <= nBurnIn:\n        raise ValueError(\"Number of warm-up iterations must be smaller than total iterations.\")\n\n    n, p = X.shape\n\n    # Frequentist initialization using statsmodels\n    import statsmodels.api as sm\n    model = sm.Logit(y, X)\n    result = model.fit(disp=0)\n    beta_start = result.params\n    variance = result.cov_params()\n\n    beta = np.zeros((nIter, p))\n    beta[0] = beta_start\n\n    prior_cov_inv = np.linalg.inv(prior_cov)\n\n    for i in range(1, nIter):\n        beta_hat = multivariate_normal.rvs(mean=beta[i-1], cov=delta * variance)\n        # Log-prior (multivariate normal)\n        log_prior_hat = multivariate_normal.logpdf(beta_hat, mean=prior_mean, cov=prior_cov)\n        log_prior_prev = multivariate_normal.logpdf(beta[i-1], mean=prior_mean, cov=prior_cov)\n        # Log acceptance ratio (posterior is proportional to likelihood * prior)\n        logR = (\n            logit_loglik(beta_hat, y, X) + log_prior_hat\n            - logit_loglik(beta[i-1], y, X) - log_prior_prev\n        )\n        if np.random.uniform() <= np.exp(logR):\n            beta[i] = beta_hat\n        else:\n            beta[i] = beta[i-1]\n\n    # Remove warm-up iterations\n    beta_post = beta[nBurnIn:]\n\n    # Plotting\n    if plot:\n        fig, axs = plt.subplots(2, p, figsize=(4*p, 8))\n        # Traceplots\n        for j in range(p):\n            axs[0, j].plot(beta_post[:, j])\n            axs[0, j].set_title(f'Traceplot: beta{j}' if j > 0 else 'Traceplot: alpha')\n        # Density plots\n        for j in range(p):\n            axs[1, j].hist(beta_post[:, j], bins=50, density=True, alpha=0.6)\n            kde = gaussian_kde(beta_post[:, j])\n            x_vals = np.linspace(np.min(beta_post[:, j]), np.max(beta_post[:, j]), 200)\n            axs[1, j].plot(x_vals, kde(x_vals), 'r')\n            axs[1, j].set_title(f'Density: beta{j}' if j > 0 else 'Density: alpha')\n        plt.tight_layout()\n        plt.show()\n\n    # Posterior summaries\n    param_names = [f'alpha'] + [f'beta{j}' for j in range(1, p)]\n    summary = pd.DataFrame({\n        'Parameter': param_names,\n        'Median': np.median(beta_post, axis=0),\n        'SD': np.std(beta_post, axis=0)\n    })\n\n    # Set column names\n    beta_post_df = pd.DataFrame(beta_post, columns=param_names)\n\n    output = {\n        'posteriorSummary': summary,\n        'posteriorSamples': beta_post_df\n    }\n    return output\n\n# Example usage\n\n## Simulate data\nn = 100\np = 3\nnp.random.seed(0)\nX = np.hstack((np.ones((n, 1)), np.random.randn(n, p - 1)))  # intercept + two predictors\nbeta_true = np.array([0.5, -1.0, 2.0])\neta = X @ beta_true\nprob = 1 / (1 + np.exp(-eta))\ny = np.random.binomial(1, prob, size=n)\n\n## Prior: mean 0, covariance diag(10^2) - this is a non-informative prior (mean 0 and high variance)\nprior_mean = np.zeros(p)\nprior_cov = np.eye(p) * 100\n\n### Run the sampler ###\noutput = MHLogitNormalPrior(\n    nIter = 10000,         # total iterations\n    nBurnIn = 5000,        # burn-in\n    y = y,\n    X = X,\n    delta = 1.0,          # proposal scale\n    prior_mean = prior_mean,\n    prior_cov = prior_cov,\n    plot = True           # show trace and density plots\n)\n\nprint(output['posteriorSummary'])","type":"content","url":"/notebooks/logit-mcmc#role-in-posterior-sampling","position":13},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal"},"type":"lvl1","url":"/notebooks/non-standard-normal","position":0},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal"},"content":"","type":"content","url":"/notebooks/non-standard-normal","position":1},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal"},"type":"lvl1","url":"/notebooks/non-standard-normal#a-mcmc-implementation-for-a-non-standard-normal","position":2},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal"},"content":"This algorithm samples from the posterior distribution of the mean (\\mu) and precision (\\tau = 1/\\sigma^2) for a normal model, given a set of observed data and conjugate priors.","type":"content","url":"/notebooks/non-standard-normal#a-mcmc-implementation-for-a-non-standard-normal","position":3},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal","lvl2":"Model Setup"},"type":"lvl2","url":"/notebooks/non-standard-normal#model-setup","position":4},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal","lvl2":"Model Setup"},"content":"Likelihood:For data x_1, \\dots, x_n assumed iid normal:x_i \\sim \\mathcal{N}(\\mu, \\tau^{-1})\n\nThe likelihood (joint density) is:p(\\mathbf{x} | \\mu, \\tau) = \\prod_{i=1}^n \\left[ \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}(x_i - \\mu)^2\\right) \\right]\n\nPriors:\n\nFor \\mu:\\mu \\sim \\mathcal{N}(\\mu_0, \\tau_0^{-1})p(\\mu) = \\sqrt{\\frac{\\tau_0}{2\\pi}} \\exp\\left(-\\frac{\\tau_0}{2}(\\mu - \\mu_0)^2\\right)\n\nFor \\tau (precision):\\tau \\sim \\text{Gamma}(a, b)p(\\tau) = \\frac{b^a}{\\Gamma(a)} \\tau^{a-1} \\exp(-b\\tau)\n\nPosterior:p(\\mu, \\tau | \\mathbf{x}) \\propto p(\\mathbf{x} | \\mu, \\tau)\\, p(\\mu)\\, p(\\tau)","type":"content","url":"/notebooks/non-standard-normal#model-setup","position":5},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal","lvl2":"Algorithm Steps and Equations"},"type":"lvl2","url":"/notebooks/non-standard-normal#algorithm-steps-and-equations","position":6},{"hierarchy":{"lvl1":"A MCMC implementation for a non-standard normal","lvl2":"Algorithm Steps and Equations"},"content":"Update \\mu (Metropolis-Hastings step):\n\nProposal: Sample \\mu^* \\sim \\mathcal{N}(\\mu, \\delta^2)\n\nAcceptance ratio:Compute the log-acceptance ratio:\\log R = \\left[ \\log p(\\mathbf{x}|\\mu^*,\\tau) + \\log p(\\mu^*) \\right] - \\left[ \\log p(\\mathbf{x}|\\mu,\\tau) + \\log p(\\mu) \\right]\n\nWhere\\log p(\\mathbf{x}|\\mu,\\tau) = \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(2\\pi/\\tau) - \\frac{\\tau}{2}(x_i-\\mu)^2 \\right)\\log p(\\mu) = -\\frac{1}{2}\\log(2\\pi/\\tau_0) - \\frac{\\tau_0}{2}(\\mu - \\mu_0)^2\n\nThe code implements these terms using exponentials and logs for numerical stability.\n\nDecision:Accept \\mu^* if \\log U \\leq \\log R, where U \\sim \\text{Uniform}(0,1).\n\nUpdate \\tau (Gibbs step):\n\nThe conditional posterior for \\tau is:\\tau | \\mu, \\mathbf{x} \\sim \\text{Gamma} \\left(a + \\frac{n}{2},\\ b + \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2 \\right)\n\nThe code samples \\tau from this gamma distribution at each iteration.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\n\ndef MHnormal(sample, priors, nIter, delta):\n    # Monitors\n    mon_mu = np.zeros(nIter)\n    mon_tau = np.zeros(nIter)\n\n    # Initial values\n    mu = np.mean(sample)\n    tau = 1/np.var(sample)\n\n    # Priors\n    mu0 = priors['mu0']\n    tau0 = priors['tau0']\n    a = priors['a']\n    b = priors['b']\n    n = len(sample)\n\n    for i in range(nIter):\n        # Proposal for mu\n        mu_hat = np.random.normal(mu, delta)\n        # Log acceptance ratio\n        logR = (\n            np.sum(\n                np.log(\n                    np.exp(-0.5 * tau * (sample - mu_hat)**2) / np.sqrt(2 * np.pi / tau)\n                )\n            )\n            - np.sum(\n                np.log(\n                    np.exp(-0.5 * tau * (sample - mu)**2) / np.sqrt(2 * np.pi / tau)\n                )\n            )\n            + np.log(\n                np.exp(-0.5 * tau0 * (mu_hat - mu0)**2) / np.sqrt(2 * np.pi / tau0)\n            )\n            - np.log(\n                np.exp(-0.5 * tau0 * (mu - mu0)**2) / np.sqrt(2 * np.pi / tau0)\n            )\n        )\n        logU = np.log(np.random.uniform(0, 1))\n        if logU <= logR:\n            mu = mu_hat\n\n        # Gibbs step for tau (shape, rate)\n        tau = np.random.gamma(a + n / 2, 1 / (b + 0.5 * np.sum((sample - mu) ** 2)))\n\n        # Store\n        mon_mu[i] = mu\n        mon_tau[i] = tau\n\n    output = pd.DataFrame({'mu': mon_mu, 'tau': mon_tau})\n\n    # Plotting\n    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n    # Traceplots\n    axs[0, 0].plot(output['mu'])\n    axs[0, 0].set_title(r'$\\mu$ trace')\n    axs[0, 0].set_ylabel('value')\n    \n    axs[0, 1].plot(1 / np.sqrt(output['tau']))\n    axs[0, 1].set_title(r'$\\sigma$ trace')\n    axs[0, 1].set_ylabel('value')\n    \n    # Density plots\n    mu_density = stats.gaussian_kde(output['mu'])\n    mu_vals = np.linspace(min(output['mu']), max(output['mu']), 200)\n    axs[1, 0].plot(mu_vals, mu_density(mu_vals), label='Posterior Density')\n    axs[1, 0].set_title(r'$\\mu$ density')\n    axs[1, 0].set_xlabel('Value')\n    axs[1, 0].set_ylabel('Density')\n    \n    sigma_samples = 1 / np.sqrt(output['tau'])\n    sigma_density = stats.gaussian_kde(sigma_samples)\n    sigma_vals = np.linspace(min(sigma_samples), max(sigma_samples), 200)\n    axs[1, 1].plot(sigma_vals, sigma_density(sigma_vals), label='Posterior Density')\n    axs[1, 1].set_title(r'$\\sigma$ density')\n    axs[1, 1].set_xlabel('Value')\n    axs[1, 1].set_ylabel('Density')\n    \n    plt.tight_layout()\n    plt.show()\n\n    return output\n\n# Example usage:\nnp.random.seed(42)\nsample = np.random.normal(40, 13, 100)\npriors = {\n    'mu0': np.mean(sample),\n    'tau0': float(np.std(sample)),\n    'a': 0.01,\n    'b': 0.01\n}\n#print(sample)\n\noutput = MHnormal(sample = sample, priors = priors, nIter = 500000, delta = 1)","type":"content","url":"/notebooks/non-standard-normal#algorithm-steps-and-equations","position":7},{"hierarchy":{"lvl1":"A MCMC implementation for a Bayesian Poisson Regression"},"type":"lvl1","url":"/notebooks/poisson-mcmc","position":0},{"hierarchy":{"lvl1":"A MCMC implementation for a Bayesian Poisson Regression"},"content":"","type":"content","url":"/notebooks/poisson-mcmc","position":1},{"hierarchy":{"lvl1":"A MCMC implementation for a Bayesian Poisson Regression"},"type":"lvl1","url":"/notebooks/poisson-mcmc#a-mcmc-implementation-for-a-bayesian-poisson-regression","position":2},{"hierarchy":{"lvl1":"A MCMC implementation for a Bayesian Poisson Regression"},"content":"","type":"content","url":"/notebooks/poisson-mcmc#a-mcmc-implementation-for-a-bayesian-poisson-regression","position":3},{"hierarchy":{"lvl1":"Model Setup"},"type":"lvl1","url":"/notebooks/poisson-mcmc#model-setup","position":4},{"hierarchy":{"lvl1":"Model Setup"},"content":"Data: Counts y_i \\in \\{0, 1, 2, \\dots\\} and predictors X_i for i = 1, \\dots, n.y_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\log \\lambda_i = X_i \\beta\n\nwhere X_i is the vector of predictors for observation i, and \\beta is the vector of regression coefficients.","type":"content","url":"/notebooks/poisson-mcmc#model-setup","position":5},{"hierarchy":{"lvl1":"Likelihood"},"type":"lvl1","url":"/notebooks/poisson-mcmc#likelihood","position":6},{"hierarchy":{"lvl1":"Likelihood"},"content":"The likelihood for the observed data, given \\beta and X, is based on the Poisson Probability Mass Function (PMF) since this is a discrete distribution:p(\\mathbf{y} | \\beta, X) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n\nwhere \\lambda_i = \\exp(X_i \\beta).\n\nThe log-likelihood:\\log p(\\mathbf{y} | \\beta, X) = \\sum_{i=1}^n \\left( y_i (X_i\\beta) - \\exp(X_i\\beta) \\right) + \\text{const}","type":"content","url":"/notebooks/poisson-mcmc#likelihood","position":7},{"hierarchy":{"lvl1":"Prior"},"type":"lvl1","url":"/notebooks/poisson-mcmc#prior","position":8},{"hierarchy":{"lvl1":"Prior"},"content":"A multivariate normal (Gaussian) prior is placed on \\beta:\\beta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\n\nwhere:\n\n\\mu_0 is the prior mean vector (often zeros),\n\n\\Sigma_0 is the prior covariance matrix.\n\nThe prior density is:p(\\beta) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma_0|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\beta - \\mu_0)^\\top \\Sigma_0^{-1} (\\beta - \\mu_0)\\right)","type":"content","url":"/notebooks/poisson-mcmc#prior","position":9},{"hierarchy":{"lvl1":"Posterior"},"type":"lvl1","url":"/notebooks/poisson-mcmc#posterior","position":10},{"hierarchy":{"lvl1":"Posterior"},"content":"The posterior combines likelihood and prior:p(\\beta | \\mathbf{y}, X) \\propto p(\\mathbf{y} | \\beta, X) \\cdot p(\\beta)","type":"content","url":"/notebooks/poisson-mcmc#posterior","position":11},{"hierarchy":{"lvl1":"Metropolis-Hastings Algorithm Steps in the Code"},"type":"lvl1","url":"/notebooks/poisson-mcmc#metropolis-hastings-algorithm-steps-in-the-code","position":12},{"hierarchy":{"lvl1":"Metropolis-Hastings Algorithm Steps in the Code"},"content":"Initialization (Frequentist/ML):\n\nThe chain starts at the MLE from a standard Poisson GLM fit.\n\nProposal Step:\n\nPropose \\beta^* from a multivariate normal centered at the current value:\n\\beta^* \\sim \\mathcal{N}(\\beta^{(t-1)}, \\delta V)\nwhere V is an estimate of the covariance matrix from the frequentist fit and \\delta is a tuning parameter.\n\nAcceptance Ratio:\n\nCompute the log of the Metropolis-Hastings acceptance ratio:\n\\log R = \\left[ \\log p(\\mathbf{y} | \\beta^*, X) + \\log p(\\beta^*) \\right] - \\left[ \\log p(\\mathbf{y} | \\beta^{(t-1)}, X) + \\log p(\\beta^{(t-1)}) \\right]\n\nAccept \\beta^* with probability \\min(1, \\exp(\\log R)).\n\nUpdate Step:\n\nIf accepted, set \\beta^{(t)} = \\beta^*; otherwise, set \\beta^{(t)} = \\beta^{(t-1)}.\n\nRepeat for nIter iterations (with multiple chains if desired), discarding nBurnIn as burn-in.","type":"content","url":"/notebooks/poisson-mcmc#metropolis-hastings-algorithm-steps-in-the-code","position":13},{"hierarchy":{"lvl1":"Additional chains"},"type":"lvl1","url":"/notebooks/poisson-mcmc#additional-chains","position":14},{"hierarchy":{"lvl1":"Additional chains"},"content":"The code below also implements multiple independent Markov Chains. This is standard practice in Bayesian estimation so parameter convergence can be validated across multiple chains besides inside a single chain.","type":"content","url":"/notebooks/poisson-mcmc#additional-chains","position":15},{"hierarchy":{"lvl1":"Model Setup"},"type":"lvl1","url":"/notebooks/poisson-mcmc#model-setup-1","position":16},{"hierarchy":{"lvl1":"Model Setup"},"content":"Data: Counts y_i \\in \\{0, 1, 2, \\dots\\} and predictors X_i for i = 1, \\dots, n.\n\nModel: Poisson regression (log-linear model)\ny_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\log \\lambda_i = X_i \\beta\nwhere X_i is the vector of predictors for observation i, and \\beta is the vector of regression coefficients.","type":"content","url":"/notebooks/poisson-mcmc#model-setup-1","position":17},{"hierarchy":{"lvl1":"Likelihood"},"type":"lvl1","url":"/notebooks/poisson-mcmc#likelihood-1","position":18},{"hierarchy":{"lvl1":"Likelihood"},"content":"The likelihood for the observed data, given \\beta and X, is based on the Poisson Probability Mass Function (PMF) since this is a discrete distribution:p(\\mathbf{y} | \\beta, X) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n\nwhere \\lambda_i = \\exp(X_i \\beta).\n\nThe log-likelihood:\\log p(\\mathbf{y} | \\beta, X) = \\sum_{i=1}^n \\left( y_i (X_i\\beta) - \\exp(X_i\\beta) \\right) + \\text{const}","type":"content","url":"/notebooks/poisson-mcmc#likelihood-1","position":19},{"hierarchy":{"lvl1":"Prior"},"type":"lvl1","url":"/notebooks/poisson-mcmc#prior-1","position":20},{"hierarchy":{"lvl1":"Prior"},"content":"A multivariate normal (Gaussian) prior is placed on \\beta:\\beta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\n\nwhere:\n\n\\mu_0 is the prior mean vector (often zeros),\n\n\\Sigma_0 is the prior covariance matrix.\n\nThe prior density is:p(\\beta) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma_0|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\beta - \\mu_0)^\\top \\Sigma_0^{-1} (\\beta - \\mu_0)\\right)","type":"content","url":"/notebooks/poisson-mcmc#prior-1","position":21},{"hierarchy":{"lvl1":"Posterior"},"type":"lvl1","url":"/notebooks/poisson-mcmc#posterior-1","position":22},{"hierarchy":{"lvl1":"Posterior"},"content":"The posterior combines likelihood and prior:p(\\beta | \\mathbf{y}, X) \\propto p(\\mathbf{y} | \\beta, X) \\cdot p(\\beta)","type":"content","url":"/notebooks/poisson-mcmc#posterior-1","position":23},{"hierarchy":{"lvl1":"Metropolis-Hastings Algorithm Steps in the Code"},"type":"lvl1","url":"/notebooks/poisson-mcmc#metropolis-hastings-algorithm-steps-in-the-code-1","position":24},{"hierarchy":{"lvl1":"Metropolis-Hastings Algorithm Steps in the Code"},"content":"Initialization (Frequentist/ML):\n\nThe chain starts at the MLE from a standard Poisson GLM fit.\n\nProposal Step:\n\nPropose \\beta^* from a multivariate normal centered at the current value:\n\\beta^* \\sim \\mathcal{N}(\\beta^{(t-1)}, \\delta V)\nwhere V is an estimate of the covariance matrix from the frequentist fit and \\delta is a tuning parameter.\n\nAcceptance Ratio:\n\nCompute the log of the Metropolis-Hastings acceptance ratio:\n\\log R = \\left[ \\log p(\\mathbf{y} | \\beta^*, X) + \\log p(\\beta^*) \\right] - \\left[ \\log p(\\mathbf{y} | \\beta^{(t-1)}, X) + \\log p(\\beta^{(t-1)}) \\right]\n\nAccept \\beta^* with probability \\min(1, \\exp(\\log R)).\n\nUpdate Step:\n\nIf accepted, set \\beta^{(t)} = \\beta^*; otherwise, set \\beta^{(t)} = \\beta^{(t-1)}.\n\nRepeat for nIter iterations (with multiple chains if desired), discarding nBurnIn as burn-in.","type":"content","url":"/notebooks/poisson-mcmc#metropolis-hastings-algorithm-steps-in-the-code-1","position":25},{"hierarchy":{"lvl1":"Additional chains"},"type":"lvl1","url":"/notebooks/poisson-mcmc#additional-chains-1","position":26},{"hierarchy":{"lvl1":"Additional chains"},"content":"The code below also implements multiple independent Markov Chains. This is standard practice in Bayesian estimation so parameter convergence can be validated across multiple chains besides inside a single chain.\n\nimport numpy as np\nfrom scipy.stats import multivariate_normal, gaussian_kde\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\n\ndef pois_loglik(beta, y, X):\n    \"\"\"\n    Computes log-likelihood for a Poisson log-linear model\n    beta: shape (,p)\n    y: integer-valued response\n    X: design matrix of covariates\n    Returns: log-likelihood\n    \"\"\"\n    beta = np.atleast_2d(beta)\n    n_chain = beta.shape[0]\n    pll = np.zeros(n_chain)\n\n    for i in range(n_chain):\n        mu = X @ beta[i]\n        pll[i] = np.sum(mu * y - np.exp(mu))\n\n    return pll if n_chain > 1 else pll[0]\n    # end of log-likelihood function\n\ndef MHpoisGLM(y, X, nIter, nBurnIn, delta, nChains=2, plot=True):\n    if nIter <= nBurnIn:\n        raise ValueError(\"Number of warm-up iterations must be smaller than total iterations.\")\n\n    n, p = X.shape\n    # Frequentist init\n    poisson_mod = sm.GLM(y, X, family=sm.families.Poisson())\n    freqMod = poisson_mod.fit()\n    beta_start = freqMod.params\n    variance = freqMod.cov_params()\n\n    # Prepare chains\n    chains = [np.zeros((nIter, p)) for _ in range(nChains)]\n    for chain in chains:\n        chain[0] = beta_start\n\n    # Run chains\n    for c in range(nChains):\n        for i in range(1, nIter):\n            beta_hat = multivariate_normal.rvs(mean=chains[c][i-1], cov=delta * variance)\n            logR = pois_loglik(beta_hat, y, X) - pois_loglik(chains[c][i-1], y, X)\n            if np.random.uniform() <= np.exp(logR):\n                chains[c][i] = beta_hat\n            else:\n                chains[c][i] = chains[c][i-1]\n\n    # Remove burn-in\n    chains_post = [chain[nBurnIn:] for chain in chains]\n\n    # Posterior summaries (for chain 1)\n    param_names = [f'alpha'] + [f'beta{j}' for j in range(1, p)]\n    summary = pd.DataFrame({\n        'Parameter': param_names,\n        'Median': np.median(chains_post[0], axis=0),\n        'SD': np.std(chains_post[0], axis=0)\n    })\n\n    # Plotting\n    if plot:\n        fig, axs = plt.subplots(2, p, figsize=(4*p, 8))\n        colors = ['black', '#FF000088', 'blue', 'green', 'orange', 'purple']\n        # Traceplots\n        for j in range(p):\n            for c in range(nChains):\n                axs[0, j].plot(chains_post[c][:, j], color=colors[c % len(colors)], alpha=0.7, label=f\"Chain {c+1}\" if j == 0 else None)\n            axs[0, j].set_title(f'Trace: beta{j}' if j > 0 else 'Trace: alpha')\n            if j == 0:\n                axs[0, j].legend()\n        # Density plots\n        for j in range(p):\n            for c in range(nChains):\n                kde = gaussian_kde(chains_post[c][:, j])\n                x_vals = np.linspace(np.min(chains_post[c][:, j]), np.max(chains_post[c][:, j]), 200)\n                axs[1, j].plot(x_vals, kde(x_vals), color=colors[c % len(colors)], label=f\"Chain {c+1}\")\n            axs[1, j].set_title(f'Density: beta{j}' if j > 0 else 'Density: alpha')\n            axs[1, j].legend()\n        plt.tight_layout()\n        plt.show()\n\n    # Prepare output\n    chains_post_df = [pd.DataFrame(chain, columns=param_names) for chain in chains_post]\n    output = {\n        'posteriorSummary': summary,\n        'chains': chains_post_df\n    }\n    return output\n\n# Example usage:\nnp.random.seed(444)\nB0 =  1.2   # intercept\nB1 =  1.5   # slope for x1\nB2 = -0.5   # slope for x2\n\ny = np.random.poisson(6.5, 100)\nx2 = np.linspace(-0.5, 0.5, len(y))\nx1 = (np.log(y+1e-2) - B0 - B2 * x2) / B1 + np.random.normal(0, 1, size=len(y))\nX = np.column_stack([np.ones(len(y)), x1, x2])\n\nresult = MHpoisGLM(y = y, X = X, nIter = 10000, nBurnIn = 1000, delta = 0.1, nChains = 4, plot = True)\nprint(result['posteriorSummary'])","type":"content","url":"/notebooks/poisson-mcmc#additional-chains-1","position":27},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model"},"type":"lvl1","url":"/notebooks/schaefer-spm-mcmc","position":0},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model"},"content":"","type":"content","url":"/notebooks/schaefer-spm-mcmc","position":1},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model"},"type":"lvl1","url":"/notebooks/schaefer-spm-mcmc#a-bayesian-implementation-of-a-schaefer-surplus-production-population-dynamics-model","position":2},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model"},"content":"","type":"content","url":"/notebooks/schaefer-spm-mcmc#a-bayesian-implementation-of-a-schaefer-surplus-production-population-dynamics-model","position":3},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Background"},"type":"lvl2","url":"/notebooks/schaefer-spm-mcmc#background","position":4},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Background"},"content":"A Surplus Production Model (SPM) describes the population biomass dynamics as a single biomass entity, conditioned on relative abundance observations. SPMs are based on theta-logistic density-dependent population growth theory, where population growth rates are higher at low abundance, and lower when approaching the carrying capacity.\n\nFor a SPM in discrete time, the biomass (B) at a given point in time (usually years) can be described as a function of the biomass, surplus production (P), and catch (C) at the previous year.B_{t} = B_{t-1} + P_{t-1} - C_{t-1}\n\nThe surplus production P (i.e. the population growth in biomass at a given point in time) is a function of two parameters: the intrinsic rate of increase (r), and carrying capacity (K):P_{t} = rB_{t}\\left(1 - \\frac{B_{t}}{K}\\right) - F_{t}B_{t}\n\nObservations are indices of relative abundance (I) scaled to biomass through (estimated) cachability coefficients q:\\hat{I} = qB_{t}","type":"content","url":"/notebooks/schaefer-spm-mcmc#background","position":5},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Model Setup"},"type":"lvl2","url":"/notebooks/schaefer-spm-mcmc#model-setup","position":6},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Model Setup"},"content":"Data: Observed catches C and one or more survey indices (columns starting with “Survey” followed by a number) over multiple years.","type":"content","url":"/notebooks/schaefer-spm-mcmc#model-setup","position":7},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Likelihood"},"type":"lvl2","url":"/notebooks/schaefer-spm-mcmc#likelihood","position":8},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Likelihood"},"content":"For each survey index, the likelihood is based on the log-normal errors between observed survey values and predicted biomass. The model assumes:\n\nThe observed survey index at time t is I_t.\n\nThe predicted biomass at time t (from the Schaefer model) is B_t.\n\nThe catchability coefficient Q scales the expected survey to the biomass:\nQ = \\exp\\left(\\frac{1}{n} \\sum_{t=1}^n \\log\\left(\\frac{I_t}{B_t}\\right)\\right)\n\nThe scaled prediction for the survey is P_t = I_t / Q.\n\nThe residuals follow a log-normal distribution:\\log\\left(\\frac{B_t}{P_t}\\right) \\sim N(0, \\sigma^2)\n\nThis means that for each year t, the probability density of observing I_t given predicted B_t and \\sigma is:p(I_t \\mid B_t, Q, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left( -\\frac{1}{2\\sigma^2} \\left[\\log\\left(\\frac{B_t}{I_t/Q}\\right)\\right]^2 \\right)\n\nFor each residual:\\text{residual}_t = \\log\\left(\\frac{B_t}{I_t/Q}\\right)\n\nandp(\\text{residual}_t \\mid \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left( -\\frac{\\text{residual}_t^2}{2\\sigma^2} \\right)\n\nThe total likelihood across years is the product of these normal pdfs across observations.","type":"content","url":"/notebooks/schaefer-spm-mcmc#likelihood","position":9},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Prior"},"type":"lvl2","url":"/notebooks/schaefer-spm-mcmc#prior","position":10},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Prior"},"content":"Priors are placed on the parameters:\n\nr \\sim N(\\mu_r, \\sigma^2_r)\n\nk \\sim N(\\mu_k, \\sigma^2_k)\n\n\\sigma \\sim N(\\mu_\\sigma, \\sigma^2_\\sigma)\n\nThese are combined into the total NLL for use in Metropolis-Hastings.","type":"content","url":"/notebooks/schaefer-spm-mcmc#prior","position":11},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Posterior"},"type":"lvl2","url":"/notebooks/schaefer-spm-mcmc#posterior","position":12},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Posterior"},"content":"The (unormalized) posterior combines the likelihood and priors:\np(r, k, \\sigma | \\text{data}) \\propto p(\\text{data} | r, k, \\sigma) \\cdot p(r) \\cdot p(k) \\cdot p(\\sigma)","type":"content","url":"/notebooks/schaefer-spm-mcmc#posterior","position":13},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Metropolis-Hastings Algorithm Steps in the Code"},"type":"lvl2","url":"/notebooks/schaefer-spm-mcmc#metropolis-hastings-algorithm-steps-in-the-code","position":14},{"hierarchy":{"lvl1":"A Bayesian implementation of a Schaefer Surplus Production population dynamics model","lvl2":"Metropolis-Hastings Algorithm Steps in the Code"},"content":"Initialization:\n\nChains are initialized at given starting values (inits).\n\nProposal Step:\n\nAt each iteration, propose new values for r, k, and \\sigma using normal random walks:\nr^* \\sim N(r^{(t-1)}, \\delta_{1}), \\quad k^* \\sim N(k^{(t-1)}, \\delta_{2}), \\quad \\sigma^* \\sim |N(\\sigma^{(t-1)}, \\delta_{2})|\n\nHard rejection if the proposed r^* \\leq 0.\n\nAcceptance Ratio:\n\nCompute the difference in negative log-likelihoods (including priors) between proposal and current values.\n\nAccept the proposal with probability \\min(1, \\exp(\\text{logR})).\n\nUpdate Step:\n\nIf accepted, set (r, k, \\sigma)^{(t)} = (r^*, k^*, \\sigma^*); otherwise, retain previous values.\n\nRepeat for nIter iterations, discarding nBurnIn as burn-in.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\ndef detect_survey_cols(data):\n    # Find all columns that start with 'Survey'\n    return [col for col in data.columns if col.lower().startswith(\"survey\")]\n\ndef NLL(par, data, survey_cols, priorMean, priorVar, printStep=False):\n    r, k, sigma = float(par[0]), float(par[1]), float(par[2])\n    nYears = len(data)\n    B = np.zeros(nYears)\n    B[0] = k\n\n    for t in range(1, nYears):\n        B[t] = max(1, B[t-1] + r * B[t-1] * (1 - B[t-1] / k) - data['C'].iloc[t-1])\n\n    # Likelihood components for each survey\n    NLLs = []\n    for survey_col in survey_cols:\n        survey_vals = data[survey_col]\n        Q   = np.exp(np.nanmean(np.log(survey_vals / B)))\n        PB  = survey_vals / Q\n        SSQ = np.log(B / PB) ** 2\n        idx = ~np.isnan(SSQ)\n        L = 1 / np.sqrt(2 * np.pi * sigma ** 2) * np.exp(-SSQ[idx] / (2 * sigma ** 2))\n        NLLs.append(-np.log(L))\n\n    # Priors\n    rPriorL     = -((r - priorMean[0]) ** 2 / (2 * priorVar[0]))\n    kPriorL     = -((k - priorMean[1]) ** 2 / (2 * priorVar[1]))\n    sigmaPriorL = -((sigma - priorMean[2]) ** 2 / (2 * priorVar[2]))\n    priorL = rPriorL + kPriorL + sigmaPriorL\n\n    # Total negative log-likelihood (minus sum for M-H log acceptance ratio)\n    totalNLL = sum(-np.sum(nll) for nll in NLLs) + priorL\n\n    if printStep:\n        print(f\"r: {r:.4f} k: {k:.2f} sigma: {sigma:.4f} nll: {totalNLL:.2f}\")\n\n    return totalNLL\n\ndef schaeferMCMC(data, inits, nIter, nBurnIn, delta, priors, nChains=2):\n    \"\"\"\n    Metropolis-Hastings sampler for the Schaefer biomass model.\n    Now uses a vector of proposal standard deviations (delta) for each parameter:\n        delta[0] = for r\n        delta[1] = for k\n        delta[2] = for sigma\n    \"\"\"\n    survey_cols = detect_survey_cols(data)\n    priorMean = priors[:3]\n    priorVar  = priors[3:]\n\n    if not (isinstance(delta, (list, tuple, np.ndarray)) and len(delta) == 3):\n        raise ValueError(\"delta must be a list/tuple/array of length 3: [delta_r, delta_k, delta_sigma]\")\n\n    chains = []\n    for chain_idx in range(nChains):\n        r     = np.zeros(nIter)\n        k     = np.zeros(nIter)\n        sigma = np.zeros(nIter)\n\n        if isinstance(inits[0], (list, np.ndarray)):\n            r[0], k[0], sigma[0] = inits[chain_idx]\n        else:\n            r[0], k[0], sigma[0] = inits\n\n        for i in range(1, nIter):\n            rHat     = np.random.normal(r[i-1], delta[0])\n            kHat     = np.random.normal(k[i-1], delta[1])\n            sigmaHat = abs(np.random.normal(sigma[i-1], delta[2]))\n\n            if rHat <= 0:\n                r[i], k[i], sigma[i] = r[i-1], k[i-1], sigma[i-1]\n                continue\n\n            logR = NLL([rHat, kHat, sigmaHat], data, survey_cols, priorMean, priorVar) - \\\n                   NLL([r[i-1], k[i-1], sigma[i-1]], data, survey_cols, priorMean, priorVar)\n\n            accept_prob = min(1, np.exp(logR))\n            if np.random.uniform() <= accept_prob:\n                r[i], k[i], sigma[i] = rHat, kHat, sigmaHat\n            else:\n                r[i], k[i], sigma[i] = r[i-1], k[i-1], sigma[i-1]\n\n        # Burn-in removal\n        rSamples     = r[nBurnIn:]\n        kSamples     = k[nBurnIn:]\n        sigmaSamples = sigma[nBurnIn:]\n\n        chains.append(pd.DataFrame({\n            'r': rSamples,\n            'k': kSamples,\n            'sigma': sigmaSamples\n        }))\n\n    # Plotting\n    fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n    colors = ['black', '#FF000088', 'blue', 'green', 'orange', 'purple']\n    # Traceplots\n    for j, param in enumerate(['r', 'k', 'sigma']):\n        for cidx, chain in enumerate(chains):\n            axs[0, j].plot(chain[param], color=colors[cidx % len(colors)], alpha=0.7, label=f'Chain {cidx+1}' if j==0 else None)\n        axs[0, j].set_title(f'Trace: {param}')\n        if j == 0:\n            axs[0, j].legend()\n    # Density plots\n    for j, param in enumerate(['r', 'k', 'sigma']):\n        for cidx, chain in enumerate(chains):\n            kde = gaussian_kde(chain[param])\n            x_vals = np.linspace(chain[param].min(), chain[param].max(), 200)\n            axs[1, j].plot(x_vals, kde(x_vals), color=colors[cidx % len(colors)], label=f'Chain {cidx+1}')\n        axs[1, j].set_title(f'Density: {param}')\n        axs[1, j].legend()\n    plt.tight_layout()\n    plt.show()\n\n    # Posterior summaries (mean and sd per chain)\n    summaries = []\n    for chain in chains:\n        summaries.append(pd.DataFrame({\n            'Mean': [chain['r'].mean(), chain['k'].mean(), chain['sigma'].mean()],\n            'SD':   [chain['r'].std(),  chain['k'].std(),  chain['sigma'].std()]\n        }, index=['r', 'k', 'sigma']))\n\n    output = {\n        'chains': chains,\n        'summaries': summaries,\n        'survey_cols': survey_cols\n    }\n    return output\n\ndef schaefer_post_plots(data, mcmc_samples, survey_cols):\n    nYears = len(data)\n    nSamples = len(mcmc_samples)\n    biomass_matrix = np.zeros((nSamples, nYears))\n\n    # Calculate biomass time series for each posterior sample\n    for s in range(nSamples):\n        r = mcmc_samples.iloc[s]['r']\n        k = mcmc_samples.iloc[s]['k']\n        B = np.zeros(nYears)\n        B[0] = k\n        for t in range(1, nYears):\n            B[t] = max(1, B[t-1] + r * B[t-1] * (1 - B[t-1] / k) - data['C'].iloc[t-1])\n        biomass_matrix[s, :] = B\n\n    biomass_mean = biomass_matrix.mean(axis=0)\n    biomass_lower = np.percentile(biomass_matrix, 2.5, axis=0)\n    biomass_upper = np.percentile(biomass_matrix, 97.5, axis=0)\n\n    # ----- SCALE THE SURVEYS -----\n    scaled_surveys = {}\n    for survey_col in survey_cols:\n        survey_vals = data[survey_col].values\n        mask = survey_vals > 0\n        # Avoid division by zero or log of zero with masking\n        q = np.exp(np.nanmean(np.log(survey_vals[mask] / biomass_mean[mask])))\n        scaled_surveys[survey_col] = survey_vals / q\n\n    # ----- Plotting -----\n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n    # 1. Biomass time series with CI and scaled surveys\n    axs[0].plot(biomass_mean, color='black', label='Mean')\n    axs[0].plot(biomass_lower, color='black', linestyle='--', label='Lower CI')\n    axs[0].plot(biomass_upper, color='black', linestyle='--', label='Upper CI')\n    colors = ['red', 'blue', 'green', 'orange', 'purple']\n    for idx, (survey_col, scaled) in enumerate(scaled_surveys.items()):\n        axs[0].plot(np.arange(nYears), scaled, 'o', label=survey_col, color=colors[idx % len(colors)])\n    axs[0].set_ylabel(\"biomass\")\n    axs[0].set_xlabel(\"Index\")\n    axs[0].legend()\n\n    # 2. B/Bmsy\n    k_samples = mcmc_samples['k'].values\n    bbmsy_matrix = biomass_matrix / (k_samples[:, None] / 2)\n    bbmsy_mean = bbmsy_matrix.mean(axis=0)\n    axs[1].plot(bbmsy_mean, color='black')\n    axs[1].set_ylabel(\"biomassSB/(k/2)\")\n    axs[1].set_xlabel(\"Index\")\n    axs[1].set_title(\"B/Bmsy\")\n\n    # 3. F/Fmsy\n    catch = data['C'].values\n    r_samples = mcmc_samples['r'].values\n    f_matrix = catch / biomass_matrix\n    fmsy_matrix = r_samples[:, None] / 2\n    ffmsy_matrix = f_matrix / fmsy_matrix\n    ffmsy_mean = ffmsy_matrix.mean(axis=0)\n    axs[2].plot(ffmsy_mean, color='black')\n    axs[2].set_ylabel(\"FM/Fmsy\")\n    axs[2].set_xlabel(\"Index\")\n    axs[2].set_title(\"F/Fmsy\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        \"biomass_mean\": biomass_mean,\n        \"biomass_lower\": biomass_lower,\n        \"biomass_upper\": biomass_upper,\n        \"scaled_surveys\": scaled_surveys\n    }\n\nYou can find some example usage below. Be warned that way more iterations (nIter and nBurnIn) are needed to achieve appropriate chain mixing and accurate parameter estimation. The example in this cookbook runs with fewer iterations due to computational limitations involved in deploying the cookbook.\n\n# --- Example usage ---\ndata = pd.read_csv('../data/data.csv')\nprint(\"Data loaded:\", data.shape) # rows, cols\n\n# Initial parameters, number of iterations, burn-in, delta, priors\ninits = [0.19, 144000, 0.9] # r, k, sigma\nnIter = 10000 # reduce for testing\nnBurnIn = 1000\ndelta = [0.0075, 20000, 0.05] # proposal distribution variances\npriors = [0.23, 144000, 0.9, 0.005, 50000, 0.01]  # priors: means (1:3) and variances (4:6)\n\nresult = schaeferMCMC(data, inits, nIter, nBurnIn, delta, priors)\n\n# Print means and SDs for each chain\nfor i, summary in enumerate(result['summaries']):\n    print(f\"Chain {i+1} parameter means and SDs:\\n{summary}\\n\")\n\nschaefer_post_plots(data, result['chains'][1], result['survey_cols'])","type":"content","url":"/notebooks/schaefer-spm-mcmc#metropolis-hastings-algorithm-steps-in-the-code","position":15},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution"},"type":"lvl1","url":"/notebooks/standard-normal-mcmc","position":0},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution"},"content":"","type":"content","url":"/notebooks/standard-normal-mcmc","position":1},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution"},"type":"lvl1","url":"/notebooks/standard-normal-mcmc#a-mcmc-implementation-for-a-standard-normal-distribution","position":2},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution"},"content":"","type":"content","url":"/notebooks/standard-normal-mcmc#a-mcmc-implementation-for-a-standard-normal-distribution","position":3},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution","lvl2":"Target Distribution (Posterior)"},"type":"lvl2","url":"/notebooks/standard-normal-mcmc#target-distribution-posterior","position":4},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution","lvl2":"Target Distribution (Posterior)"},"content":"In notebook, the goal is to generate samples from the standard normal distribution (a normal distribution centered around 0 with standard deviation of 1):p(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)\n\nSince there is no data (I’m not conditioning on any observations for this simple example), the posterior is simply the target distribution p(x), the standard normal density.","type":"content","url":"/notebooks/standard-normal-mcmc#target-distribution-posterior","position":5},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution","lvl2":"Likelihood"},"type":"lvl2","url":"/notebooks/standard-normal-mcmc#likelihood","position":6},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution","lvl2":"Likelihood"},"content":"For the standard normal, when simulating from the distribution (not fitting to data), the likelihood term is just the value of the target density itself:L(x) = p(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)","type":"content","url":"/notebooks/standard-normal-mcmc#likelihood","position":7},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution","lvl2":"MCMC steps"},"type":"lvl2","url":"/notebooks/standard-normal-mcmc#mcmc-steps","position":8},{"hierarchy":{"lvl1":"A MCMC implementation for a standard normal distribution","lvl2":"MCMC steps"},"content":"Proposal Step:\n\nPropose a new point x^* from a normal distribution centered at the current x:\nx^* \\sim \\mathcal{N}(x^{(t-1)}, \\sigma^2)\n\nThis is a random walk Metropolis proposal: q(x^* | x^{(t-1)}) = \\mathcal{N}(x^{(t-1)}, \\sigma^2).\n\nAcceptance Ratio:\n\nSince the proposal distribution is symmetric, the q terms cancel out in the general M-H acceptance ratio:\n\\alpha = \\min\\left(1, \\frac{p(x^*)}{p(x^{(t-1)})}\\right)\n\nPlugging in the normal density:\n\\alpha = \\min\\left(1, \\frac{\\exp\\left(-\\frac{(x^*)^2}{2}\\right)}{\\exp\\left(-\\frac{(x^{(t-1)})^2}{2}\\right)}\\right)\n= \\min\\left(1, \\exp\\left(-\\frac{1}{2}\\left[(x^*)^2 - (x^{(t-1)})^2\\right]\\right)\\right)\n\nUpdate Step:\n\nAccept x^* with probability \\alpha. Otherwise, keep the current value.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numba import njit\n\ndef MHSTDnormal(N, X0, sigmaSq):\n    x = np.empty(N)\n    x[0] = X0\n    for i in range(1, N):\n        xhat = np.random.normal(x[i-1], np.sqrt(sigmaSq))\n        accept_prob = np.exp(-0.5 * (xhat**2 - x[i-1]**2))\n        if np.random.uniform() <= accept_prob:\n            x[i] = xhat\n        else:\n            x[i] = x[i-1]\n    return x\n\nnormalSamples = MHSTDnormal(N = 10000, # number of iterations\n                            X0 = 0.3,  # starting value\n                            sigmaSq = 10**1 # variance of the standard normal (sd = 1, var = 1^2)\n                           )\n\nplt.plot(normalSamples)\nplt.title('Metropolis-Hastings samples from Standard Normal')\nplt.xlabel('Iteration')\nplt.ylabel('Sample Value')\nplt.show()\n\nplt.figure(figsize = (8,4))\ncount, bins, ignored = plt.hist(normalSamples, bins = 50, density = True, alpha = 0.6, color = \"skyblue\")\n\nx = np.linspace(-4, 4, 1000)\nplt.plot(x, 1/np.sqrt(2*np.pi) * np.exp(-x**2/2), 'r', lw=2, label='True Standard Normal')\n\nplt.title('Posterior Distribution')\nplt.xlabel('Sample Value')\nplt.ylabel('Density')\nplt.legend()\nplt.show()","type":"content","url":"/notebooks/standard-normal-mcmc#mcmc-steps","position":9}]}