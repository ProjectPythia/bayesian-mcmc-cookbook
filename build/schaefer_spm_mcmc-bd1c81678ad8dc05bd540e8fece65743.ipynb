{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95c5ab6-5618-4664-b498-a485b2d7f670",
   "metadata": {},
   "source": [
    "### A Bayesian implementation of a Schaefer Surplus Production population dynamics model\n",
    "\n",
    "#### **Background**\n",
    "\n",
    "A Surplus Production Model (SPM) describes the population biomass dynamics as a single biomass entity, conditioned on relative abundance observations. SPMs are based on theta-logistic density-dependent population growth theory, where population growth rates are higher at low abundance, and lower when approaching the carrying capacity.\n",
    "\n",
    "For a SPM in discrete time, the biomass (B) at a given point in time (usually years) can be described as a function of the biomass, surplus production (P), and catch (C) at the previous year.\n",
    "\n",
    "$$\n",
    "B_{t} = B_{t-1} + P_{t-1} - C_{t-1}\n",
    "$$\n",
    "\n",
    "The surplus production $P$ (i.e. the population growth in biomass at a given point in time) is a function of two parameters: the intrinsic rate of increase ($r$), and carrying capacity (K):\n",
    "\n",
    "$$\n",
    "P_{t} = rB_{t}\\left(1 - \\frac{B_{t}}{K}\\right) - F_{t}B_{t}\n",
    "$$\n",
    "\n",
    "Observations are indices of relative abundance ($I$) scaled to biomass through (estimated) cachability coefficients $q$:\n",
    "\n",
    "$$\n",
    "\\hat{I} = qB_{t}\n",
    "$$\n",
    "\n",
    "#### **Model Setup**\n",
    "\n",
    "- **Data:** Observed catches `C` and one or more survey indices (columns starting with \"Survey\" followed by a number) over multiple years.\n",
    "\n",
    "#### **Likelihood**\n",
    "\n",
    "For each survey index, the likelihood is based on the log-normal errors between observed survey values and predicted biomass. The model assumes:\n",
    "\n",
    "- The observed survey index at time $t$ is $I_t$.\n",
    "- The predicted biomass at time $t$ (from the Schaefer model) is $B_t$.\n",
    "- The catchability coefficient $Q$ scales the expected survey to the biomass:\n",
    "  $$\n",
    "  Q = \\exp\\left(\\frac{1}{n} \\sum_{t=1}^n \\log\\left(\\frac{I_t}{B_t}\\right)\\right)\n",
    "  $$\n",
    "- The scaled prediction for the survey is $P_t = I_t / Q$.\n",
    "\n",
    "The residuals follow a log-normal distribution:\n",
    "  $$\n",
    "  \\log\\left(\\frac{B_t}{P_t}\\right) \\sim N(0, \\sigma^2)\n",
    "  $$\n",
    "\n",
    "This means that for each year $t$, the probability density of observing $I_t$ given predicted $B_t$ and $\\sigma$ is:\n",
    "  $$\n",
    "  p(I_t \\mid B_t, Q, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left( -\\frac{1}{2\\sigma^2} \\left[\\log\\left(\\frac{B_t}{I_t/Q}\\right)\\right]^2 \\right)\n",
    "  $$\n",
    "\n",
    "For each residual:\n",
    "  $$\n",
    "  \\text{residual}_t = \\log\\left(\\frac{B_t}{I_t/Q}\\right)\n",
    "  $$\n",
    "and\n",
    "  $$\n",
    "  p(\\text{residual}_t \\mid \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\left( -\\frac{\\text{residual}_t^2}{2\\sigma^2} \\right)\n",
    "  $$\n",
    "\n",
    "The **total likelihood** across years is the product of these normal pdfs across observations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Prior**\n",
    "\n",
    "- Priors are placed on the parameters:\n",
    "  - $r \\sim N(\\mu_r, \\sigma^2_r)$\n",
    "  - $k \\sim N(\\mu_k, \\sigma^2_k)$\n",
    "  - $\\sigma \\sim N(\\mu_\\sigma, \\sigma^2_\\sigma)$\n",
    "- These are combined into the total NLL for use in Metropolis-Hastings.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Posterior**\n",
    "\n",
    "- The (unormalized) posterior combines the likelihood and priors:\n",
    "  $$\n",
    "  p(r, k, \\sigma | \\text{data}) \\propto p(\\text{data} | r, k, \\sigma) \\cdot p(r) \\cdot p(k) \\cdot p(\\sigma)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Metropolis-Hastings Algorithm Steps in the Code**\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Chains are initialized at given starting values (`inits`).\n",
    "\n",
    "2. **Proposal Step:**\n",
    "   - At each iteration, propose new values for $r$, $k$, and $\\sigma$ using normal random walks:\n",
    "     $$\n",
    "     r^* \\sim N(r^{(t-1)}, \\delta_{1}), \\quad k^* \\sim N(k^{(t-1)}, \\delta_{2}), \\quad \\sigma^* \\sim |N(\\sigma^{(t-1)}, \\delta_{2})|\n",
    "     $$\n",
    "   - Hard rejection if the proposed $r^* \\leq 0$.\n",
    "\n",
    "3. **Acceptance Ratio:**\n",
    "   - Compute the difference in negative log-likelihoods (including priors) between proposal and current values.\n",
    "   - Accept the proposal with probability $\\min(1, \\exp(\\text{logR}))$.\n",
    "\n",
    "4. **Update Step:**\n",
    "   - If accepted, set $(r, k, \\sigma)^{(t)} = (r^*, k^*, \\sigma^*)$; otherwise, retain previous values.\n",
    "   - Repeat for `nIter` iterations, discarding `nBurnIn` as burn-in.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b37c0ae-da4e-4168-bae5-7d94305b8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4da7c7-655c-4410-bc5f-2786a88aa327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_survey_cols(data):\n",
    "    # Find all columns that start with 'Survey'\n",
    "    return [col for col in data.columns if col.lower().startswith(\"survey\")]\n",
    "\n",
    "def NLL(par, data, survey_cols, priorMean, priorVar, printStep=False):\n",
    "    r, k, sigma = float(par[0]), float(par[1]), float(par[2])\n",
    "    nYears = len(data)\n",
    "    B = np.zeros(nYears)\n",
    "    B[0] = k\n",
    "\n",
    "    for t in range(1, nYears):\n",
    "        B[t] = max(1, B[t-1] + r * B[t-1] * (1 - B[t-1] / k) - data['C'].iloc[t-1])\n",
    "\n",
    "    # Likelihood components for each survey\n",
    "    NLLs = []\n",
    "    for survey_col in survey_cols:\n",
    "        survey_vals = data[survey_col]\n",
    "        Q   = np.exp(np.nanmean(np.log(survey_vals / B)))\n",
    "        PB  = survey_vals / Q\n",
    "        SSQ = np.log(B / PB) ** 2\n",
    "        idx = ~np.isnan(SSQ)\n",
    "        L = 1 / np.sqrt(2 * np.pi * sigma ** 2) * np.exp(-SSQ[idx] / (2 * sigma ** 2))\n",
    "        NLLs.append(-np.log(L))\n",
    "\n",
    "    # Priors\n",
    "    rPriorL     = -((r - priorMean[0]) ** 2 / (2 * priorVar[0]))\n",
    "    kPriorL     = -((k - priorMean[1]) ** 2 / (2 * priorVar[1]))\n",
    "    sigmaPriorL = -((sigma - priorMean[2]) ** 2 / (2 * priorVar[2]))\n",
    "    priorL = rPriorL + kPriorL + sigmaPriorL\n",
    "\n",
    "    # Total negative log-likelihood (minus sum for M-H log acceptance ratio)\n",
    "    totalNLL = sum(-np.sum(nll) for nll in NLLs) + priorL\n",
    "\n",
    "    if printStep:\n",
    "        print(f\"r: {r:.4f} k: {k:.2f} sigma: {sigma:.4f} nll: {totalNLL:.2f}\")\n",
    "\n",
    "    return totalNLL\n",
    "\n",
    "def schaeferMCMC(data, inits, nIter, nBurnIn, delta, priors, nChains=2):\n",
    "    \"\"\"\n",
    "    Metropolis-Hastings sampler for the Schaefer biomass model.\n",
    "    Now uses a vector of proposal standard deviations (delta) for each parameter:\n",
    "        delta[0] = for r\n",
    "        delta[1] = for k\n",
    "        delta[2] = for sigma\n",
    "    \"\"\"\n",
    "    survey_cols = detect_survey_cols(data)\n",
    "    priorMean = priors[:3]\n",
    "    priorVar  = priors[3:]\n",
    "\n",
    "    if not (isinstance(delta, (list, tuple, np.ndarray)) and len(delta) == 3):\n",
    "        raise ValueError(\"delta must be a list/tuple/array of length 3: [delta_r, delta_k, delta_sigma]\")\n",
    "\n",
    "    chains = []\n",
    "    for chain_idx in range(nChains):\n",
    "        r     = np.zeros(nIter)\n",
    "        k     = np.zeros(nIter)\n",
    "        sigma = np.zeros(nIter)\n",
    "\n",
    "        if isinstance(inits[0], (list, np.ndarray)):\n",
    "            r[0], k[0], sigma[0] = inits[chain_idx]\n",
    "        else:\n",
    "            r[0], k[0], sigma[0] = inits\n",
    "\n",
    "        for i in range(1, nIter):\n",
    "            rHat     = np.random.normal(r[i-1], delta[0])\n",
    "            kHat     = np.random.normal(k[i-1], delta[1])\n",
    "            sigmaHat = abs(np.random.normal(sigma[i-1], delta[2]))\n",
    "\n",
    "            if rHat <= 0:\n",
    "                r[i], k[i], sigma[i] = r[i-1], k[i-1], sigma[i-1]\n",
    "                continue\n",
    "\n",
    "            logR = NLL([rHat, kHat, sigmaHat], data, survey_cols, priorMean, priorVar) - \\\n",
    "                   NLL([r[i-1], k[i-1], sigma[i-1]], data, survey_cols, priorMean, priorVar)\n",
    "\n",
    "            accept_prob = min(1, np.exp(logR))\n",
    "            if np.random.uniform() <= accept_prob:\n",
    "                r[i], k[i], sigma[i] = rHat, kHat, sigmaHat\n",
    "            else:\n",
    "                r[i], k[i], sigma[i] = r[i-1], k[i-1], sigma[i-1]\n",
    "\n",
    "        # Burn-in removal\n",
    "        rSamples     = r[nBurnIn:]\n",
    "        kSamples     = k[nBurnIn:]\n",
    "        sigmaSamples = sigma[nBurnIn:]\n",
    "\n",
    "        chains.append(pd.DataFrame({\n",
    "            'r': rSamples,\n",
    "            'k': kSamples,\n",
    "            'sigma': sigmaSamples\n",
    "        }))\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    colors = ['black', '#FF000088', 'blue', 'green', 'orange', 'purple']\n",
    "    # Traceplots\n",
    "    for j, param in enumerate(['r', 'k', 'sigma']):\n",
    "        for cidx, chain in enumerate(chains):\n",
    "            axs[0, j].plot(chain[param], color=colors[cidx % len(colors)], alpha=0.7, label=f'Chain {cidx+1}' if j==0 else None)\n",
    "        axs[0, j].set_title(f'Trace: {param}')\n",
    "        if j == 0:\n",
    "            axs[0, j].legend()\n",
    "    # Density plots\n",
    "    for j, param in enumerate(['r', 'k', 'sigma']):\n",
    "        for cidx, chain in enumerate(chains):\n",
    "            kde = gaussian_kde(chain[param])\n",
    "            x_vals = np.linspace(chain[param].min(), chain[param].max(), 200)\n",
    "            axs[1, j].plot(x_vals, kde(x_vals), color=colors[cidx % len(colors)], label=f'Chain {cidx+1}')\n",
    "        axs[1, j].set_title(f'Density: {param}')\n",
    "        axs[1, j].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Posterior summaries (mean and sd per chain)\n",
    "    summaries = []\n",
    "    for chain in chains:\n",
    "        summaries.append(pd.DataFrame({\n",
    "            'Mean': [chain['r'].mean(), chain['k'].mean(), chain['sigma'].mean()],\n",
    "            'SD':   [chain['r'].std(),  chain['k'].std(),  chain['sigma'].std()]\n",
    "        }, index=['r', 'k', 'sigma']))\n",
    "\n",
    "    output = {\n",
    "        'chains': chains,\n",
    "        'summaries': summaries,\n",
    "        'survey_cols': survey_cols\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def schaefer_post_plots(data, mcmc_samples, survey_cols):\n",
    "    nYears = len(data)\n",
    "    nSamples = len(mcmc_samples)\n",
    "    biomass_matrix = np.zeros((nSamples, nYears))\n",
    "\n",
    "    # Calculate biomass time series for each posterior sample\n",
    "    for s in range(nSamples):\n",
    "        r = mcmc_samples.iloc[s]['r']\n",
    "        k = mcmc_samples.iloc[s]['k']\n",
    "        B = np.zeros(nYears)\n",
    "        B[0] = k\n",
    "        for t in range(1, nYears):\n",
    "            B[t] = max(1, B[t-1] + r * B[t-1] * (1 - B[t-1] / k) - data['C'].iloc[t-1])\n",
    "        biomass_matrix[s, :] = B\n",
    "\n",
    "    biomass_mean = biomass_matrix.mean(axis=0)\n",
    "    biomass_lower = np.percentile(biomass_matrix, 2.5, axis=0)\n",
    "    biomass_upper = np.percentile(biomass_matrix, 97.5, axis=0)\n",
    "\n",
    "    # ----- SCALE THE SURVEYS -----\n",
    "    scaled_surveys = {}\n",
    "    for survey_col in survey_cols:\n",
    "        survey_vals = data[survey_col].values\n",
    "        mask = survey_vals > 0\n",
    "        # Avoid division by zero or log of zero with masking\n",
    "        q = np.exp(np.nanmean(np.log(survey_vals[mask] / biomass_mean[mask])))\n",
    "        scaled_surveys[survey_col] = survey_vals / q\n",
    "\n",
    "    # ----- Plotting -----\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # 1. Biomass time series with CI and scaled surveys\n",
    "    axs[0].plot(biomass_mean, color='black', label='Mean')\n",
    "    axs[0].plot(biomass_lower, color='black', linestyle='--', label='Lower CI')\n",
    "    axs[0].plot(biomass_upper, color='black', linestyle='--', label='Upper CI')\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for idx, (survey_col, scaled) in enumerate(scaled_surveys.items()):\n",
    "        axs[0].plot(np.arange(nYears), scaled, 'o', label=survey_col, color=colors[idx % len(colors)])\n",
    "    axs[0].set_ylabel(\"biomass\")\n",
    "    axs[0].set_xlabel(\"Index\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # 2. B/Bmsy\n",
    "    k_samples = mcmc_samples['k'].values\n",
    "    bbmsy_matrix = biomass_matrix / (k_samples[:, None] / 2)\n",
    "    bbmsy_mean = bbmsy_matrix.mean(axis=0)\n",
    "    axs[1].plot(bbmsy_mean, color='black')\n",
    "    axs[1].set_ylabel(\"biomassSB/(k/2)\")\n",
    "    axs[1].set_xlabel(\"Index\")\n",
    "    axs[1].set_title(\"B/Bmsy\")\n",
    "\n",
    "    # 3. F/Fmsy\n",
    "    catch = data['C'].values\n",
    "    r_samples = mcmc_samples['r'].values\n",
    "    f_matrix = catch / biomass_matrix\n",
    "    fmsy_matrix = r_samples[:, None] / 2\n",
    "    ffmsy_matrix = f_matrix / fmsy_matrix\n",
    "    ffmsy_mean = ffmsy_matrix.mean(axis=0)\n",
    "    axs[2].plot(ffmsy_mean, color='black')\n",
    "    axs[2].set_ylabel(\"FM/Fmsy\")\n",
    "    axs[2].set_xlabel(\"Index\")\n",
    "    axs[2].set_title(\"F/Fmsy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"biomass_mean\": biomass_mean,\n",
    "        \"biomass_lower\": biomass_lower,\n",
    "        \"biomass_upper\": biomass_upper,\n",
    "        \"scaled_surveys\": scaled_surveys\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e0ec5-890a-4b54-9022-71181e9f5f18",
   "metadata": {},
   "source": [
    "You can find some example usage below. Be warned that way more iterations (`nIter` and `nBurnIn`) are needed to achieve appropriate chain mixing and accurate parameter estimation. The example in this cookbook runs with fewer iterations due to computational limitations involved in deploying the cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e47ea-07ea-4b64-b4a0-91b6d24b6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (106, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "data = pd.read_csv('../data/data.csv')\n",
    "print(\"Data loaded:\", data.shape) # rows, cols\n",
    "\n",
    "# Initial parameters, number of iterations, burn-in, delta, priors\n",
    "inits = [0.19, 144000, 0.9] # r, k, sigma\n",
    "nIter = 10000 # reduce for testing\n",
    "nBurnIn = 1000\n",
    "delta = [0.0075, 20000, 0.05] # proposal distribution variances\n",
    "priors = [0.23, 144000, 0.9, 0.005, 50000, 0.01]  # priors: means (1:3) and variances (4:6)\n",
    "\n",
    "result = schaeferMCMC(data, inits, nIter, nBurnIn, delta, priors)\n",
    "\n",
    "# Print means and SDs for each chain\n",
    "for i, summary in enumerate(result['summaries']):\n",
    "    print(f\"Chain {i+1} parameter means and SDs:\\n{summary}\\n\")\n",
    "\n",
    "schaefer_post_plots(data, result['chains'][1], result['survey_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b3be9-c69c-4ce7-9c7b-e36b03f9458e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
